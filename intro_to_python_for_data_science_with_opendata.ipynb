{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Intro to Python for Data Science with DC OpenData</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dc flag](./images/4994-004-096A5339.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>This notebook was prepared by [Nicole Donnelly](mailto:nicole.donnelly@dc.gov) for the DC area regional Women in Data Science Conference [(DCMDVAWiDSRegional)](https://sites.google.com/view/dcmdvawidsregional/agenda?authuser=0) on March 5,2018 and presented as a one hour workshop.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "As cities embrace the [open data](https://en.wikipedia.org/wiki/Open_data) movement (you can find links to datasets for 85 cities [here](https://www.forbes.com/sites/metabrown/2017/06/30/quick-links-to-municipal-open-data-portals-for-85-us-cities/#290b91072290)), data scientist have an ever expanding population of data available to analyze and incorporate into other projects. As with any data source, unless you are designing and collecting it yourself, you will likely need to do some data wrangling before moving on to exploratory data analysis (EDA) and machine learning. \n",
    "\n",
    "During the course of this workshop, we will look at using [Python](https://www.python.org/) to wrangle [open data available from the Government of the District of Columbia](http://opendata.dc.gov/) in preparation for machine learning (this workshop will not cover machine learning). We will also look at some initial EDA via visualizations once we build a data set we want to use.\n",
    "\n",
    "### Overview\n",
    "If you do not have particular project in mind, I encourage you to [browse through the available data sets](http://opendata.dc.gov/datasets) (951 as of the time this workshop was created). We are going to start today with the [Computer Assisted Mass Appraisal - Condominium](http://opendata.dc.gov/datasets/computer-assisted-mass-appraisal-condominium) data. There is a lot that can be done with this data, particularly in conjunction with other data available from DC ([tax data](http://opendata.dc.gov/datasets/integrated-tax-system-public-extract), [crime data](http://opendata.dc.gov/datasets?q=crime), [construction data](http://opendata.dc.gov/datasets?q=construction), or [city service requests](http://opendata.dc.gov/datasets?q=311) for example) or other sources like the [United States Census Bureau](https://www.census.gov/data.html).\n",
    "\n",
    "Buying a house in DC can be a daunting task. Inventory was being describe in November 2017 as [\"dismally low\"](https://www.washingtonpost.com/news/where-we-live/wp/2017/11/14/buyers-are-gaining-more-leverage-in-the-hot-d-c-area-housing-market/?utm_term=.b1aa57960214). But maybe armed with some appraisal data and machine learning, we can understand condominium values a little better. For example, maybe we could create a simple application to determine appraisal value, similar to [this example](https://github.com/georgetown-analytics/machine-learning/blob/master/examples/bbengfort/home%20sales/home_sales.ipynb) which uses housing sales data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Here is [some information](https://www.arcgis.com/sharing/rest/content/items/d6c70978daa8461992658b69dccb3dbf/info/metadata/metadata.xml?format=default&output=html) available to us about the data.\n",
    "\n",
    "**Abstract**: Computer Assisted Mass Appraisal (CAMA) database. The dataset contains attribution on housing characteristics for commercial properties, and was created as part of the DC Geographic Information System (DC GIS) for the D.C. Office of the Chief Technology Officer (OCTO) and participating D.C. government agencies. All DC GIS data is stored and exported in Maryland State Plane coordinates NAD 83 meters. \n",
    "\n",
    "METADATA CONTENT IS IN PROCESS OF VALIDATION AND SUBJECT TO CHANGE.\n",
    "\n",
    "**Purpose**: This data is used for the planning and management of Washington, D.C. by local government agencies.\n",
    "\n",
    "**Supplemental Information**: Most lots have one building in the cama file, assigned BLDG_NUM of one in the table. For parcels where multiple buildings exist, the primary building (such as the main residence) is assigned BLDG_NUM = 1. The other buildings or structures have BLDG_NUM values in random sequential order. After the primary structure, there is no way to associate BLDG_NUM > 2 records with any particular structure on the lot.\n",
    "\n",
    "\n",
    "\n",
    "There is also some attribute information available. Some of it has been copied here. Not all of it is overly descriptive. \n",
    "\n",
    "\n",
    "***Entity and Attribute Information***:\n",
    "\n",
    "\n",
    "**Attribute Label**: SALEDATE\n",
    "\n",
    "**Attribute**:\n",
    "\n",
    "\n",
    "**Attribute Label**: Sale_Num\n",
    "\n",
    "**Attribute Definition**: sale number\n",
    "\n",
    "\n",
    "**Attribute Label**: EYB\n",
    "\n",
    "**Attribute Definition:** The calculated or apparent year, that an improvement was built that is most often more recent than actual year built.\n",
    "\n",
    "\n",
    "**Attribute Label**: Shape\n",
    "\n",
    "**Attribute Definition**: Feature geometry.\n",
    "\n",
    "\n",
    "**Attribute Label**: OWNERNAME\n",
    "\n",
    "**Attribute Definition**: property owner name\n",
    "\n",
    "\n",
    "**Attribute Label**: SSL\n",
    "\n",
    "**Attribute Definition**: square suffix and lot\n",
    "\n",
    "\n",
    "**Attribute Label**: Extwall_D\n",
    "\n",
    "**Attribute Definition**: exterior wall description\n",
    "\n",
    "\n",
    "**Attribute Label**: PRICE\n",
    "\n",
    "**Attribute**:\n",
    "\n",
    "\n",
    "**Attribute Label**: Yr_Rmdl\n",
    "\n",
    "**Attribute Definition**: year structure was remodeled\n",
    "\n",
    "\n",
    "**Attribute Label**: Saledate\n",
    "\n",
    "**Attribute Definition**: date of last sale\n",
    "\n",
    "\n",
    "**Attribute Label**: AYB\n",
    "\n",
    "**Attribute Definition**: The earliest time the main portion of the building was built. It is not affected by subsequent construction.\n",
    "\n",
    "\n",
    "**Attribute Label**: Price\n",
    "\n",
    "**Attribute Definition**: price of last sale\n",
    "\n",
    "\n",
    "**Attribute Label**: GBA\n",
    "\n",
    "**Attribute Definition**: gross building area in square feet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "This workshop has been created in [Jupyter Notebook](http://jupyter.org/) with [Python 3.6](https://www.python.org/downloads/release/python-360/). If you are unfamiliar with how to use a Jupyter notebook, consult [this tutorial](https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/).\n",
    "\n",
    "A popular package for working with data in python is [pandas](https://pandas.pydata.org/pandas-docs/stable/).\n",
    "\n",
    "From the above link:\n",
    "\n",
    "\"**pandas** is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, **real world** data analysis in Python. Additionally, it has the broader goal of becoming **the most powerful and flexible open source data analysis / manipulation tool available in any language**. It is already well on its way toward this goal.\n",
    "\n",
    "pandas is well suited for many different kinds of data:\n",
    "\n",
    "* Tabular data with heterogeneously-typed columns, as in an SQL table or Excel spreadsheet\n",
    "* Ordered and unordered (not necessarily fixed-frequency) time series data.\n",
    "* Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels\n",
    "* Any other form of observational / statistical data sets. The data actually need not be labeled at all to be placed into a pandas data structure\"\n",
    "\n",
    "We will also use [Seaborn](https://seaborn.pydata.org/) which is a visualization package built on [matplotlib](https://matplotlib.org/), a 2D plotting library in python. [openpyxl](https://pypi.python.org/pypi/openpyxl) is used to assist with writing files to an excel format for an optional part of the work below.\n",
    "\n",
    "The following libraries, which are part of standard python, are also used:\n",
    "\n",
    "* [os](https://docs.python.org/3/library/os.html)\n",
    "* [urllib](https://docs.python.org/3/library/urllib.html)\n",
    "\n",
    "After importing the packages and libraries needed, I am also setting two options that will assist in this exercise. `pd.options.display.max_columns = 35` is a pandas option that controls the number of columns displayed in a dataframe. Here we override the default and display 35. `%matplotlib inline` tells the notebook to display our plots in the notebook instead of in an external window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import openpyxl\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 35\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling\n",
    "\n",
    "Now we are ready to get our data. Storing the original files is always good practice so you can go back to the original data if needed. Below you will find a short python function to download data to a data directory. This way, if you decide to download more data later, you can set the information particular to that data as variables and re-use this function.\n",
    "\n",
    "(There are a lot of ways data wrangling can be approached. Below is just one example.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a default data directory. Since we are using os, this convention will work on both Windows and\n",
    "# *nix based environments\n",
    "DATA_DIR = \"./data\"\n",
    "\n",
    "# the two variables below are the url of our dataset on the opendata site and the path/name for the file\n",
    "#we are downloading\n",
    "cama_url = \"https://opendata.arcgis.com/datasets/d6c70978daa8461992658b69dccb3dbf_24.csv\"\n",
    "cama_file = os.path.join(DATA_DIR, \"cama-condo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the get_data function takes 3 variables - our data directory (dname), the url of our data (furl) and \n",
    "# the path/name for our file (fname)\n",
    "\n",
    "def get_data(dname, furl, fname):\n",
    "    \n",
    "    # check to see if the data directory exists. if not, create it and print the message\n",
    "    if not os.path.exists(dname):\n",
    "        print(\"making directory\")\n",
    "        os.makedirs(dname)\n",
    "    # if the data directory exists, just print the message\n",
    "    else:\n",
    "        print(\"directory exists\")\n",
    "        \n",
    "    # check to see if the file exists. if not, download the file and print the message\n",
    "    if not os.path.isfile(fname):\n",
    "        print(\"downloading file\")\n",
    "        urllib.request.urlretrieve(furl, fname)\n",
    "    \n",
    "    # if the file exists, print the message (if you cloned the github repo, all the data is included)\n",
    "    else:\n",
    "        print(\"file exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the get_data function to download and save Computer Assisted Mass Appraisal - Condominium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(DATA_DIR, cama_url, cama_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to use pandas to read the downloaded csv file into a dataframe called \"df\". There are a lot of options you can use when creating a dataframe. Take a look at the [documenation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(cama_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use \"head\" to see the head, or first 5 rows of the data. If you want to see more than 5, put the number of rows you would like to see more rows, put the number you want inside the parenthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape will tell us the shape -- number of rows and columns -- in our dataset. Columns will give us a list of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info will give us even more info. Take a few minutes to review the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some data, let's start wrangling it in to something useful. If we use the idea mentioned above to create a simple application that uses machine learning to determine the appraisal value of a condominium in order to assist with the daunting task of purchasing in DC, we might want to start wrangling the dataset into something we can use to perform a [regression analysis](https://en.wikipedia.org/wiki/Regression_analysis). With this in mind, we will need data that contains numerical variables and we will also want to look at the correlations among these variables.\n",
    "\n",
    "To add some additional information to help us work through the data, let's say our hypothetical buyer has decided they are only interested in condos with at least 2 bedrooms. Additionally, they would like to live in [Ward 6](https://planning.dc.gov/page/about-ward-6). The land area isn't really important to them.\n",
    "\n",
    "Let's start by dropping some items we won't need. OBJECTID is a unique idea in the data and is not going to be useful in our regression. We don't have good information in our metadata on what QUALIFIED or USECODE are. We hadve decided LANDAREA is not important. GIS_LAST_MOD_DTTM is a modification date for the data, so also not relevant here. We use drop to drop columns in pandas. The axis variable of 1 specifies columns. \"inplace\" tells pandas we want to drop the columns from our actual dataframe. Without it, pandas will return a temporary dataframe object that excludes those columns. Again, the [documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html) will explain the options available for the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['OBJECTID', 'QUALIFIED', 'USECODE', 'LANDAREA', 'GIS_LAST_MOD_DTTM'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SALEDATE is probably something we want to hold on to. But as info showed us, the data is an object, not a date/time. Let's convert it to something more useable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['SALEDATE'] = pd.to_datetime(df['SALEDATE'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our data looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number after the column name tells us the number of non-null values. It looks like we don't have a price for everything. PRICE has 48,934 non-null values while our data has 52,954. An object isn't going to be useful to us without a price and we don't really have an easy way to figure out what that missing price should be. So let's drop those items from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df.PRICE.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have some missing values. Our hypothetical buyer wants to buy something with at least 2 bedrooms so next let's drop condos with less than 2 bedrooms from our data. We can do this by subsetting our dataframe to only items where the value in BEDRM >= 2. We then assign that to a variable we can call. By re-using our variable df, we are overwriting the data we are already referencing it with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df.BEDRM >= 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's look at the column YR_RMDL. There are also a lot of null values and we have no meaningful way to impute them. Let's drop this column.\n",
    "\n",
    "Our buyer isn't really concerned with the overall number of rooms as long as the condo has 2 bedrooms so we can drop that column as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['YR_RMDL', 'ROOMS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HF_BATHRM and FIREPLACES also contain null values. However, instead of dropping those columns, let's assume a null value is equivalent to 0. We can use fillna to change all our null values to 0. We can address HEAT and HEAT_D if we decide to use those in our regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our buyer wants to live in Ward 6. How can we figure out which of the condos are located there? SSL (square suffix and lot) actually gives us a key to do that. We can determine the address from SSL. Once we have the addrss, we can get additional useful information, including Ward. \n",
    "\n",
    "Let's start with the address.\n",
    "\n",
    "[Address Residential Units](http://opendata.dc.gov/datasets/address-residential-units): This table contains residential units and attributes of Address points, created as part of the Master Address Repository (MAR) for the D.C. Residential units can be condominiums or also apartments. Office of the Chief Technology Officer (OCTO) and DC Department of Consumer and Regulatory Affairs . It contains the addresses in the District of Columbia which are typically placed on the buildings. More information on the MAR can be found at http://dcgis.dc.gov.\n",
    "\n",
    "We can download this data using our get_data function then read it into a dataframe, just like with our CAMA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aru_url = \"https://opendata.arcgis.com/datasets/c3c0ae91dca54c5d9ce56962fa0dd645_68.csv\"\n",
    "aru_file = os.path.join(DATA_DIR, \"address_residential_unit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(DATA_DIR, aru_url, aru_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aru_df = pd.read_csv(aru_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aru_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aru_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aru_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSL is common to both our datasets. Unfortunately, we seem to have a lot of null values in SSL with the address data. How many matches do we have between the two? There is a way to easily check that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SSL'].isin(aru_df['SSL']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas gives us a way to connect dataframes with merge. Here we are creating a third dataframe, condos, by merging our initial dataframe, df, with our address dataframe, aru_df. We also tell pandas that SSL is the common information between the two dataframes. By default, pandas performs an [inner join](https://www.codeproject.com/Articles/33052/Visual-Representation-of-SQL-Joins). You could also use a [left, right, or outer](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "condos = pd.merge(df, aru_df, on='SSL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condos.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful thing we can look at is what values exist in our data, and how many times they occur in a particular column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(condos.UNITTYPE.value_counts())\n",
    "print('\\n')\n",
    "print(condos.STATUS.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset didn't come with all that much information about it. Let's make the assumption we only want items that are ACTIVE. Then let's drop some columnds we don't need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "condos = condos[condos.STATUS != 'RETIRE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "condos.drop(['OBJECTID', 'STATUS', 'UNITTYPE', 'METADATA_ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condos.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have addresses our units, but we don't know what Ward anything is in. The address dataset gave us an important bit of inforamation though. DC has a [Master Address Repository(MAR)](https://octo.dc.gov/node/1161947). And in the MAR [user guide](https://octo.dc.gov/sites/default/files/dc/sites/octo/publication/attachments/DCGIS-MARGeocoderUserGuide_1.pdf) we find out that we can get the Ward and a lot of other interesting things from the MAR. Unfortunately, the MAR application is only available for Windows 7 and 10.\n",
    "\n",
    "In order to add the MAR information to our data, we can process our address via Access or Excel and run it as a batch. We currently have 19,563 condo units in our dataset. But since these are condo units, the number of unique address is lower so it makes sense to create a dataframe with our unique address, save those to and Excel file, and only process those address with the MAR application. I have done that separately and have included the resulting MAR encoded file in the github repo. I have included the steps here. If you would like to do this as well, you can change the following cell to a code cell and run it. We will need the mar_file variable later so run that bit of code regardless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a variable with the path/name of the file that will contain your unique address list\n",
    "mar_file = os.path.join(DATA_DIR, \"addresses.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "# change this cell if you would like to perform the MAR encoding steps yourself\n",
    "\n",
    "# create a writer object for the excel file\n",
    "writer = pd.ExcelWriter(mar_file)\n",
    "\n",
    "# create a dataframe that is just the unique addresses from our condos dataframe\n",
    "addresses = pd.DataFrame(condos['FULLADDRESS'].unique(), columns=['full_address'])\n",
    "\n",
    "# write the dataframe to excel and save the file\n",
    "addresses.to_excel(writer, index=False)\n",
    "writer.save()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information on the options for the MAR encoder are found in its documention. When I ran the unique address list, it took about 5 minutes. \n",
    "\n",
    "We can read the MAR encoded data back into a dataframe and join the data to our condos dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mar = pd.read_excel(mar_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mar.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "condos = pd.merge(condos, mar, left_on='FULLADDRESS',  right_on='full_address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condos.MAR_WARD.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the Ward data we need. Let's create another dataframe, condo_6, that contains the condos for Ward 6. Then drop the columns we won't need. Let's keep MAR_CENSUS_TRACT in case we want to use that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "condo_6 = condos[condos.MAR_WARD == 'Ward 6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_6.drop(['full_address',  'MAR_MATCHADDRESS', 'MAR_XCOORD', 'MAR_YCOORD', 'MAR_LATITUDE', 'MAR_LONGITUDE', \n",
    "              'MAR_WARD', 'MAR_ZIPCODE', 'MARID', 'MAR_ERROR', 'MAR_SCORE', 'MAR_SOURCEOPERATION', \n",
    "              'MAR_IGNORE'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_6.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "Congratulations! You have just wrangled a dataset!\n",
    "\n",
    "We set up a hypotheical scenario at the beginning of this workshop that guided our data wrangling. This is not the only thing that can be done with this data, nor is this the only way to perform data wrangling. Hopefully it was at least illustrative of things to thing about all the way.\n",
    "\n",
    "\n",
    "All of those steps have lead us to a point where we can start doing some EDA and even machine learning on our data. Even though machine learning is outside the scope of this workshop, it is worth noting that if you are going to perform machine learning with something like [scikit-learn](http://scikit-learn.org/stable/) you will need to have numeric data. We do have a few columns with non-numeric data we would need to deal with before then, either by encoding the data to numeric values or dropping it from the data our models will use. \n",
    "\n",
    "We will leave those for now while we look at some ways to learn about our data.\n",
    "\n",
    "Describe in pandas will provide us with some descriptive statistics about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_6.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you learn about the data using describe?\n",
    "\n",
    "It looks like in Ward 6, the average condo has 2.1 bedrooms, 1.8 bathrooms, has been sold twice, costs $460,416 dollars and has a living area of 1115.7 square feet. \n",
    "\n",
    "Seaborn gives us a lot of options to visualize the data. Let's look at the distribution of the sale price for the condos. By default, seaborn displays a histogram with the [kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE) on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "g = sns.distplot(condo_6.PRICE, rug=True, kde=True, ax=ax)\n",
    "t = g.set_title(\"Distribution of Sale Prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would also be interesting to understand the price based on the year the unit was last sold. We have the SALEDATE and can use that to group the sales prices by year in a [box plot](https://en.wikipedia.org/wiki/Box_plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,8))\n",
    "g = sns.boxplot(y='PRICE', x=condo_6['SALEDATE'].dt.year, data=condo_6, ax=ax)\n",
    "t = g.set_title(\"Distribution of Sale Price by Year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn's joint plot allows you to view both a joint distribution and its marginals at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(y=\"PRICE\", x=\"LIVING_GBA\", data=condo_6, kind=\"hex\", size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we started with the idea of performing a regression with the data, we can also look at the correlations between our numeric variables in order to identify whether our not we have highly correlated variables. To do that, we will create (yet) another dataframe with the numerical variables. We can create a correlation matrix and provide that to the seaborn heatmap function to create a visual representation of correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = condo_6[list(set(condo_6.columns) - set(['SSL', 'SALEDATE', 'FULLADDRESS', 'UNITNUM']))]\n",
    "numerical.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = numerical.corr()\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(corr_matrix, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I hope you found this introduction to python a useful starting point for getting started with open data for data science projects. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:widsc]",
   "language": "python",
   "name": "conda-env-widsc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
